{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8lHpezSp__z",
        "outputId": "777456a1-cb25-4710-b771-416993af2c34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkVRToWke-8-",
        "outputId": "7cbcfaa7-0a7e-4054-91a6-ac908c9a5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AuIG7oPhe-9E"
      },
      "outputs": [],
      "source": [
        "text_file = \"/content/drive/MyDrive/corpus_100k.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GjlVztnZe-9E"
      },
      "outputs": [],
      "source": [
        "with open(text_file, 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6wzg_VR4e-9F",
        "outputId": "2131351e-dd1a-45f5-b377-f2b2a7168535"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Հիմա ես կարող է մի բառ մի քիչ անզգույշ արտահայտեմ , եւ երեք ամիս ՀՀ ներքաղաքական կյանքն այդպես էլ չխ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IxSFdh9te-9G"
      },
      "outputs": [],
      "source": [
        "def replace_punctuation(text):\n",
        "\n",
        "  punctuation = re.compile(r\"[^\\w\\s]\")\n",
        "  text = punctuation.sub(\" \", text)\n",
        "\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "  pattern = re.compile(r\"\\s+\")\n",
        "  text = pattern.sub(\" \", text)\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = replace_punctuation(text)\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qugn72yjoxQZ",
        "outputId": "9ddb9f42-9548-41bf-a206-64360e73ff0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Հիմա ես կարող է մի բառ մի քիչ անզգույշ արտահայտեմ եւ երեք ամիս ՀՀ ներքաղաքական կյանքն այդպես էլ չխաղ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZysJkoBme-9H"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(text, vocabulary_size):\n",
        "\n",
        "  text = text.lower()\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "\n",
        "  counter = collections.Counter(tokens)\n",
        "\n",
        "  top_n_words = []\n",
        "  for word, count in counter.most_common(vocabulary_size):\n",
        "    word = word.strip()\n",
        "    top_n_words.append(word)\n",
        "\n",
        "  return top_n_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pirYU0ebe-9H"
      },
      "outputs": [],
      "source": [
        "vocabulary = build_vocabulary(text, 2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jHVSe-rRe-9H"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/vocab.txt', 'w') as f:\n",
        "    f.write(str(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9jfXoUACe-9I"
      },
      "outputs": [],
      "source": [
        "def create_training_dataset(word_sequence, window_size):\n",
        "\n",
        "  skip_grams = []\n",
        "  for i in range(1, len(word_sequence) - 1):\n",
        "    input = word_sequence[i]\n",
        "    context = []\n",
        "    for j in range(-window_size, window_size + 1):\n",
        "      if j != 0 and 0 <= i + j < len(word_sequence):\n",
        "        context.append(word_sequence[i + j])\n",
        "\n",
        "    for w in context:\n",
        "      skip_grams.append([input, w])\n",
        "\n",
        "  return skip_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_vSLFCHe-9I",
        "outputId": "5b9c5619-f9b6-40e5-8d69-6181a046dcaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['որ', 'է'],\n",
              " ['որ', 'են'],\n",
              " ['որ', 'և'],\n",
              " ['են', 'է'],\n",
              " ['են', 'որ'],\n",
              " ['են', 'և'],\n",
              " ['են', 'եւ'],\n",
              " ['և', 'որ'],\n",
              " ['և', 'են'],\n",
              " ['և', 'եւ'],\n",
              " ['և', 'ի'],\n",
              " ['եւ', 'են'],\n",
              " ['եւ', 'և'],\n",
              " ['եւ', 'ի'],\n",
              " ['եւ', 'հետ'],\n",
              " ['ի', 'և'],\n",
              " ['ի', 'եւ'],\n",
              " ['ի', 'հետ'],\n",
              " ['ի', 'համար'],\n",
              " ['հետ', 'եւ'],\n",
              " ['հետ', 'ի'],\n",
              " ['հետ', 'համար'],\n",
              " ['հետ', 'այս'],\n",
              " ['համար', 'ի'],\n",
              " ['համար', 'հետ'],\n",
              " ['համար', 'այս'],\n",
              " ['համար', 'մասին'],\n",
              " ['այս', 'հետ'],\n",
              " ['այս', 'համար'],\n",
              " ['այս', 'մասին'],\n",
              " ['այս', 'ու'],\n",
              " ['մասին', 'համար'],\n",
              " ['մասին', 'այս'],\n",
              " ['մասին', 'ու'],\n",
              " ['մասին', 'այդ'],\n",
              " ['ու', 'այս'],\n",
              " ['ու', 'մասին'],\n",
              " ['ու', 'այդ'],\n",
              " ['ու', 'ին'],\n",
              " ['այդ', 'մասին'],\n",
              " ['այդ', 'ու'],\n",
              " ['այդ', 'ին'],\n",
              " ['այդ', 'էր'],\n",
              " ['ին', 'ու'],\n",
              " ['ին', 'այդ'],\n",
              " ['ին', 'էր'],\n",
              " ['ին', 'հհ'],\n",
              " ['էր', 'այդ'],\n",
              " ['էր', 'ին'],\n",
              " ['էր', 'հհ'],\n",
              " ['էր', 'իր'],\n",
              " ['հհ', 'ին'],\n",
              " ['հհ', 'էր'],\n",
              " ['հհ', 'իր'],\n",
              " ['հհ', 'այն'],\n",
              " ['իր', 'էր'],\n",
              " ['իր', 'հհ'],\n",
              " ['իր', 'այն'],\n",
              " ['իր', 'չի'],\n",
              " ['այն', 'հհ'],\n",
              " ['այն', 'իր'],\n",
              " ['այն', 'չի'],\n",
              " ['այն', 'թե'],\n",
              " ['չի', 'իր'],\n",
              " ['չի', 'այն'],\n",
              " ['չի', 'թե'],\n",
              " ['չի', 'մի'],\n",
              " ['թե', 'այն'],\n",
              " ['թե', 'չի'],\n",
              " ['թե', 'մի'],\n",
              " ['թե', 'նա'],\n",
              " ['մի', 'չի'],\n",
              " ['մի', 'թե'],\n",
              " ['մի', 'նա'],\n",
              " ['մի', 'եմ'],\n",
              " ['նա', 'թե'],\n",
              " ['նա', 'մի'],\n",
              " ['նա', 'եմ'],\n",
              " ['նա', 'ենք'],\n",
              " ['եմ', 'մի'],\n",
              " ['եմ', 'նա'],\n",
              " ['եմ', 'ենք'],\n",
              " ['եմ', 'պետք'],\n",
              " ['ենք', 'նա'],\n",
              " ['ենք', 'եմ'],\n",
              " ['ենք', 'պետք'],\n",
              " ['ենք', 'ոչ'],\n",
              " ['պետք', 'եմ'],\n",
              " ['պետք', 'ենք'],\n",
              " ['պետք', 'ոչ'],\n",
              " ['պետք', 'մեր'],\n",
              " ['ոչ', 'ենք'],\n",
              " ['ոչ', 'պետք'],\n",
              " ['ոչ', 'մեր'],\n",
              " ['ոչ', 'էլ'],\n",
              " ['մեր', 'պետք'],\n",
              " ['մեր', 'ոչ'],\n",
              " ['մեր', 'էլ'],\n",
              " ['մեր', 'կարող'],\n",
              " ['էլ', 'ոչ']]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "window_size = 2\n",
        "training_dataset = create_training_dataset(vocabulary, window_size)\n",
        "training_dataset[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RvoQJPdue-9I"
      },
      "outputs": [],
      "source": [
        "def random_batch(data, size, vocabulary):\n",
        "\n",
        "  if size > len(data):\n",
        "    raise ValueError(\"The size of the batch is greater than the size of the data.\")\n",
        "\n",
        "  random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "  random_inputs = []\n",
        "  random_labels = []\n",
        "  for i in random_index:\n",
        "    random_inputs.append(np.eye(len(vocabulary))[vocabulary.index(data[i][0])])  # input\n",
        "    random_labels.append(vocabulary.index(data[i][1]))  # context word\n",
        "\n",
        "  return random_inputs, random_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KLL5ZchWe-9I"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = random_batch(training_dataset,200, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aMCVFIFye-9J"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ1aI3V3zU6w",
        "outputId": "36561e9f-e570-4abb-fd70-2881f0f76edc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRzVXRPjzWop",
        "outputId": "bf1430ca-feb1-4cd5-e2a3-817b389556c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qAprDyute-9J"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2500\n",
        "embedding_dim = 2500\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sampled_softmax_loss(y_true, y_pred):\n",
        "    return tf.nn.sampled_softmax_loss(\n",
        "        weights=tf.transpose(model.layers[0].weights[0]),\n",
        "        biases=model.layers[0].weights[1],\n",
        "        labels=tf.expand_dims(y_true, -1),\n",
        "        inputs=y_pred,\n",
        "        num_sampled=5,\n",
        "        num_classes=vocab_size\n",
        "    )"
      ],
      "metadata": {
        "id": "gsACZhY13fnY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=sampled_softmax_loss, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AQVog0DxdOK",
        "outputId": "e6040ed3-24ba-45c0-9dc3-da12ba0180eb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1, 2500)           6250000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1, 2500)           6252500   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,502,500\n",
            "Trainable params: 12,502,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, batch_size=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "3yCFoJ9fxetp",
        "outputId": "157d15e4-f451-4e60-ce7e-4590f527270e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3986025d7362>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-34-47be9510cd5d>\", line 8, in sampled_softmax_loss  *\n        num_classes=vocab_size\n\n    IndexError: list index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3ARryW8x2Gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "10web_task",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}