{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUKDbTliMPTrRLW2B8/MV6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xEMmp8Xj26P5"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.Wxh = np.random.randn(vocab_size, self.hidden_size) * 0.01\n",
        "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(self.hidden_size, vocab_size) * 0.01\n",
        "        self.bh = np.zeros((self.hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    def forward(self, inputs, h_prev):\n",
        "        h_next = np.tanh(np.dot(self.Wxh.T, inputs) + np.dot(self.Whh, h_prev) + self.bh) #hidden_size x 1\n",
        "        y = np.dot(self.Why.T, h_next) + self.by # vocab_size x 1\n",
        "        probs = np.exp(y) / np.sum(np.exp(y))\n",
        "        return probs, h_next\n",
        "\n",
        "\n",
        "    def train(self, inputs, targets, epochs, lr):\n",
        "      inputs_one_hot = [self.one_hot_encode(c) for c in inputs]\n",
        "      targets_one_hot = [self.one_hot_encode(c) for c in targets]\n",
        "\n",
        "      loss_history = []\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        h_prev = np.zeros((self.hidden_size, 1))\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(len(inputs)):\n",
        "          x = inputs_one_hot[t]\n",
        "          target = targets_one_hot[t]\n",
        "\n",
        "          p, h_prev = self.forward(x, h_prev)\n",
        "\n",
        "          target_idx = np.where(target == 1)[0]\n",
        "\n",
        "          loss += -np.log(p[target_idx])\n",
        "\n",
        "          dL_dy = p #vocab_size x 1\n",
        "          dL_dy[target_idx] -= 1\n",
        "\n",
        "\n",
        "          dL_dWhy = np.dot(h_prev, dL_dy.T) #hidden_size x vocab_size\n",
        "          dL_dby = np.copy(dL_dy) #vocab_size x 1\n",
        "\n",
        "          dL_dh = np.dot(dL_dWhy, dL_dby) #hidden_size x 1\n",
        "          dL_dh_raw = (1 - h_prev * h_prev) * dL_dh\n",
        "\n",
        "          dL_dWxh = np.dot(x, dL_dh_raw.T) #vocab x hidden\n",
        "\n",
        "          dL_dWhh = np.dot(h_prev, dL_dh.T)\n",
        "          dL_dbh = dL_dh_raw\n",
        "\n",
        "          self.Wxh -= lr * dL_dWxh\n",
        "          self.Whh -= lr * dL_dWhh\n",
        "          self.Why -= lr * dL_dWhy\n",
        "          self.bh -= lr * np.sum(dL_dbh, axis=1, keepdims=True)\n",
        "          self.by -= lr * np.sum(dL_dby, axis=1, keepdims=True)\n",
        "\n",
        "        loss_history.append(loss/len(inputs))\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_history[-1]}\")\n",
        "      return loss_history\n",
        "\n",
        "    def sample(self, seed, length):\n",
        "      x = self.one_hot_encode(seed)\n",
        "      h_prev = np.zeros((self.hidden_size, 1))\n",
        "      output = seed\n",
        "\n",
        "      for _ in range(length):\n",
        "        p, h_prev = self.forward(x, h_prev)\n",
        "\n",
        "        idx = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
        "        char = self.index_to_char[idx]\n",
        "        output += char\n",
        "        x = self.one_hot_encode(char)\n",
        "      return output\n",
        "\n",
        "    def one_hot_encode(self, char):\n",
        "      vector = np.zeros((self.vocab_size, 1))\n",
        "      vector[self.char_to_index[char]] = 1\n",
        "\n",
        "      return vector"
      ],
      "metadata": {
        "id": "apivMzqp3guw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_chars = ['h', 'e', 'l', 'p']\n",
        "target_chars = ['e', 'l', 'p', '!']\n",
        "\n",
        "chars = list(set(input_chars + target_chars))\n",
        "char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
        "index_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "hidden_size = 100\n",
        "vocab_size = len(chars)\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "rnn = RNN(hidden_size, vocab_size)\n",
        "rnn.char_to_index = char_to_index\n",
        "rnn.index_to_char = index_to_char\n",
        "loss_history = rnn.train(input_chars, target_chars, num_epochs, learning_rate)\n",
        "\n",
        "seed = 'h'\n",
        "generated_sequence = rnn.sample(seed, length=4)\n",
        "print(f\"Generated Sequence: {generated_sequence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex4ktOje3xb5",
        "outputId": "fda5b3ee-d4d1-4de4-fe16-0fbd1d9c49f4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: [[1.63955575]]\n",
            "Epoch 2/100, Loss: [[1.62120632]]\n",
            "Epoch 3/100, Loss: [[1.60552031]]\n",
            "Epoch 4/100, Loss: [[1.59198321]]\n",
            "Epoch 5/100, Loss: [[1.58020636]]\n",
            "Epoch 6/100, Loss: [[1.56988768]]\n",
            "Epoch 7/100, Loss: [[1.56078797]]\n",
            "Epoch 8/100, Loss: [[1.55271535]]\n",
            "Epoch 9/100, Loss: [[1.54551431]]\n",
            "Epoch 10/100, Loss: [[1.53905769]]\n",
            "Epoch 11/100, Loss: [[1.53324072]]\n",
            "Epoch 12/100, Loss: [[1.52797648]]\n",
            "Epoch 13/100, Loss: [[1.52319239]]\n",
            "Epoch 14/100, Loss: [[1.51882748]]\n",
            "Epoch 15/100, Loss: [[1.51483026]]\n",
            "Epoch 16/100, Loss: [[1.51115699]]\n",
            "Epoch 17/100, Loss: [[1.50777037]]\n",
            "Epoch 18/100, Loss: [[1.50463842]]\n",
            "Epoch 19/100, Loss: [[1.50173358]]\n",
            "Epoch 20/100, Loss: [[1.49903205]]\n",
            "Epoch 21/100, Loss: [[1.49651315]]\n",
            "Epoch 22/100, Loss: [[1.49415886]]\n",
            "Epoch 23/100, Loss: [[1.49195342]]\n",
            "Epoch 24/100, Loss: [[1.48988299]]\n",
            "Epoch 25/100, Loss: [[1.48793538]]\n",
            "Epoch 26/100, Loss: [[1.48609981]]\n",
            "Epoch 27/100, Loss: [[1.48436675]]\n",
            "Epoch 28/100, Loss: [[1.48272768]]\n",
            "Epoch 29/100, Loss: [[1.48117505]]\n",
            "Epoch 30/100, Loss: [[1.47970206]]\n",
            "Epoch 31/100, Loss: [[1.47830263]]\n",
            "Epoch 32/100, Loss: [[1.47697131]]\n",
            "Epoch 33/100, Loss: [[1.47570314]]\n",
            "Epoch 34/100, Loss: [[1.47449369]]\n",
            "Epoch 35/100, Loss: [[1.47333889]]\n",
            "Epoch 36/100, Loss: [[1.47223508]]\n",
            "Epoch 37/100, Loss: [[1.47117892]]\n",
            "Epoch 38/100, Loss: [[1.47016736]]\n",
            "Epoch 39/100, Loss: [[1.4691976]]\n",
            "Epoch 40/100, Loss: [[1.46826708]]\n",
            "Epoch 41/100, Loss: [[1.46737346]]\n",
            "Epoch 42/100, Loss: [[1.46651457]]\n",
            "Epoch 43/100, Loss: [[1.46568843]]\n",
            "Epoch 44/100, Loss: [[1.46489319]]\n",
            "Epoch 45/100, Loss: [[1.46412715]]\n",
            "Epoch 46/100, Loss: [[1.46338874]]\n",
            "Epoch 47/100, Loss: [[1.46267649]]\n",
            "Epoch 48/100, Loss: [[1.46198905]]\n",
            "Epoch 49/100, Loss: [[1.46132515]]\n",
            "Epoch 50/100, Loss: [[1.4606836]]\n",
            "Epoch 51/100, Loss: [[1.46006332]]\n",
            "Epoch 52/100, Loss: [[1.45946326]]\n",
            "Epoch 53/100, Loss: [[1.45888247]]\n",
            "Epoch 54/100, Loss: [[1.45832005]]\n",
            "Epoch 55/100, Loss: [[1.45777514]]\n",
            "Epoch 56/100, Loss: [[1.45724696]]\n",
            "Epoch 57/100, Loss: [[1.45673475]]\n",
            "Epoch 58/100, Loss: [[1.45623781]]\n",
            "Epoch 59/100, Loss: [[1.45575549]]\n",
            "Epoch 60/100, Loss: [[1.45528715]]\n",
            "Epoch 61/100, Loss: [[1.45483221]]\n",
            "Epoch 62/100, Loss: [[1.45439011]]\n",
            "Epoch 63/100, Loss: [[1.45396033]]\n",
            "Epoch 64/100, Loss: [[1.45354236]]\n",
            "Epoch 65/100, Loss: [[1.45313573]]\n",
            "Epoch 66/100, Loss: [[1.45274001]]\n",
            "Epoch 67/100, Loss: [[1.45235475]]\n",
            "Epoch 68/100, Loss: [[1.45197956]]\n",
            "Epoch 69/100, Loss: [[1.45161407]]\n",
            "Epoch 70/100, Loss: [[1.4512579]]\n",
            "Epoch 71/100, Loss: [[1.4509107]]\n",
            "Epoch 72/100, Loss: [[1.45057216]]\n",
            "Epoch 73/100, Loss: [[1.45024195]]\n",
            "Epoch 74/100, Loss: [[1.44991979]]\n",
            "Epoch 75/100, Loss: [[1.44960537]]\n",
            "Epoch 76/100, Loss: [[1.44929844]]\n",
            "Epoch 77/100, Loss: [[1.44899874]]\n",
            "Epoch 78/100, Loss: [[1.44870601]]\n",
            "Epoch 79/100, Loss: [[1.44842002]]\n",
            "Epoch 80/100, Loss: [[1.44814054]]\n",
            "Epoch 81/100, Loss: [[1.44786736]]\n",
            "Epoch 82/100, Loss: [[1.44760027]]\n",
            "Epoch 83/100, Loss: [[1.44733908]]\n",
            "Epoch 84/100, Loss: [[1.44708358]]\n",
            "Epoch 85/100, Loss: [[1.44683361]]\n",
            "Epoch 86/100, Loss: [[1.44658899]]\n",
            "Epoch 87/100, Loss: [[1.44634955]]\n",
            "Epoch 88/100, Loss: [[1.44611513]]\n",
            "Epoch 89/100, Loss: [[1.44588557]]\n",
            "Epoch 90/100, Loss: [[1.44566073]]\n",
            "Epoch 91/100, Loss: [[1.44544047]]\n",
            "Epoch 92/100, Loss: [[1.44522465]]\n",
            "Epoch 93/100, Loss: [[1.44501314]]\n",
            "Epoch 94/100, Loss: [[1.44480582]]\n",
            "Epoch 95/100, Loss: [[1.44460255]]\n",
            "Epoch 96/100, Loss: [[1.44440323]]\n",
            "Epoch 97/100, Loss: [[1.44420775]]\n",
            "Epoch 98/100, Loss: [[1.44401599]]\n",
            "Epoch 99/100, Loss: [[1.44382785]]\n",
            "Epoch 100/100, Loss: [[1.44364323]]\n",
            "Generated Sequence: hllep\n"
          ]
        }
      ]
    }
  ]
}